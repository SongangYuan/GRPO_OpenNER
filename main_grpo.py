#!/usr/bin/env python3
"""
TRL-GRPO è®­ç»ƒå…¥å£ï¼ˆå››å¡ + Accelerate + LoRAï¼‰
å®Œå…¨åŸºäºTRLï¼Œä¸ä¾èµ–verlæ¡†æ¶
æ•°æ®æ ¼å¼ï¼š{"prompt": "...", "answer": "..."} æˆ– {"prompt": "...", "entities": [...]}
"""

import json
import re
import torch
import logging
import numpy as np
import os
from datetime import datetime
from typing import List, Union, Optional, Dict, Any
from dataclasses import dataclass, field
from collections import defaultdict, deque

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    HfArgumentParser,
    AutoModelForCausalLM,
    TrainerCallback,
)
from peft import LoraConfig, get_peft_model
from trl import GRPOTrainer, GRPOConfig

# å¯¼å…¥è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
import sys
from pathlib import Path
# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
from verl.utils.reward_score.custom_reward import compute_ner_score_v2

logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    model_name_or_path: str = field(metadata={"help": "HF æ¨¡å‹è·¯å¾„æˆ–åç§°"})
    lora_r: int = field(default=64)
    lora_alpha: int = field(default=128)

@dataclass
class DataArguments:
    dataset_path: str = field(metadata={"help": "JSONL æ•°æ®æ–‡ä»¶"})

@dataclass
class LoggingArguments:
    metrics_output_dir: Optional[str] = field(default=None, metadata={"help": "ä¿å­˜æŒ‡æ ‡JSONçš„ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ --output_dirï¼‰"})

def load_jsonl(path):
    """åŠ è½½JSON Linesæ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒåµŒå¥—æ•°ç»„æ ¼å¼ï¼‰"""
    with open(path, "r", encoding="utf-8") as f:
        content = json.load(f)
    
    # å¦‚æœæ˜¯åµŒå¥—æ•°ç»„ï¼Œå±•å¹³ä¸ºä¸€ç»´åˆ—è¡¨
    if isinstance(content, list) and all(isinstance(item, list) for item in content):
        return [{"conversation": conv} for conv in content]
    else:
        return [{"conversation": content}]  # å•æ¡å¯¹è¯

# ===== ä½¿ç”¨ custom_reward.py ä¸­çš„é«˜çº§å¥–åŠ±å‡½æ•° =====
# æ‰€æœ‰å¥–åŠ±è®¡ç®—é€»è¾‘ç°åœ¨éƒ½åœ¨ verl/utils/reward_score/custom_reward.py ä¸­

# æ–°å¢ï¼šä»assistantå›ç­”ä¸­è§£æ<ner_result>ä¸ºå®ä½“åˆ—è¡¨ï¼ˆç”¨äºæ„é€ ground truthï¼‰
def _extract_entities_from_text(text: str) -> List[str]:
    if not isinstance(text, str):
        return []
    # æŠ“å–æ ‡ç­¾å†…å®¹
    m = re.search(r"<\s*ner_result\s*>\s*(.*?)\s*<\s*/\s*ner_result\s*>", text, re.DOTALL | re.IGNORECASE)
    if not m:
        # å…œåº•ï¼šå°è¯•ä»æ–¹æ‹¬å·ä¸­æå–
        b = re.search(r"\[(.*?)\]", text, re.DOTALL)
        if not b:
            return []
        content = b.group(1)
        # å…ˆå°è¯•è§£æJSON
        try:
            arr = json.loads("[" + content + "]")
            if isinstance(arr, list):
                return [str(x).strip() for x in arr if str(x).strip()]
        except Exception:
            pass
        # å†é€€å›åˆ°å¼•å·æå–
        items = re.findall(r'"([^"]*)"', content)
        return [s.strip() for s in items if s.strip()]
    ner_content = m.group(1).strip()
    # ä¼˜å…ˆåœ¨æ ‡ç­¾å†…éƒ¨æŸ¥æ‰¾æ–¹æ‹¬å·æ•°ç»„å¹¶è§£æ
    try:
        bracket_json = re.search(r"\[\s*.*?\s*\]", ner_content, re.DOTALL)
        if bracket_json:
            arr = json.loads(bracket_json.group(0))
            if isinstance(arr, list):
                return [str(e).strip() for e in arr if str(e).strip()]
    except Exception:
        pass
    # å…ˆå°è¯•ä¸¥æ ¼JSONè§£æ
    try:
        entities = json.loads(ner_content)
        if isinstance(entities, list):
            return [str(e).strip() for e in entities if str(e).strip()]
    except Exception:
        pass
    # å›é€€ï¼šä»å¼•å·æå–
    items = re.findall(r'"([^"]*)"', ner_content)
    if items:
        return [s.strip() for s in items if s.strip()]
    # æœ€åå›é€€ï¼šå°è¯•ç”¨é€—å·åˆ†å‰²ï¼ˆå»é™¤åŒ…è£¹çš„æ–¹æ‹¬å·ä¸å¼•å·ï¼‰
    raw = ner_content.strip().strip('[]')
    if raw:
        parts = re.split(r"\s*,\s*", raw)
        ents = [p.strip().strip('"').strip("'") for p in parts if p.strip().strip('"').strip("'")]
        if ents:
            return ents
    return []

def unified_reward_function(completions, **kwargs):
     """ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°è®¡ç®—åˆ†æ•°"""
     rewards = []
     
     # è®¡ç®—ä¸completionså¯¹é½æ‰€éœ€çš„ç´¢å¼•æ˜ å°„ï¼ˆå¤„ç†æ¯ä¸ªpromptå¯èƒ½ç”Ÿæˆå¤šæ¡completionçš„æƒ…å†µï¼‰
     prompts_list = kwargs.get("prompts")
     if not isinstance(prompts_list, list):
         prompts_list = kwargs.get("prompt")
     num_prompts = len(prompts_list) if isinstance(prompts_list, list) else (1 if isinstance(prompts_list, str) else 0)
 
     for idx, completion in enumerate(completions):
         try:
             # è®¡ç®—æ ·æœ¬ç´¢å¼•ï¼šå½“æ¯ä¸ªpromptç”Ÿæˆå¤šæ¡completionæ—¶ï¼Œä½¿ç”¨å–æ¨¡å¯¹é½
             sample_idx = (idx % num_prompts) if num_prompts else idx
 
             # ä¼˜å…ˆä» reward_kwargs ä¸­æå– ground_truthï¼ˆTRLä¼šå°†è‡ªå®šä¹‰å­—æ®µæ‰“åŒ…åœ¨æ­¤å¤„ï¼‰
             rk = kwargs.get("reward_kwargs")
             rk_item = None
             if isinstance(rk, list) and len(rk) > 0:
                 rk_index = sample_idx % len(rk)
                 rk_item = rk[rk_index]
             elif isinstance(rk, dict):
                 rk_item = rk
 
             def pick_val(container, key):
                 if not isinstance(container, dict):
                     return None
                 val = container.get(key)
                 if isinstance(val, list):
                     if len(val) == 0:
                         return None
                     return val[sample_idx] if sample_idx < len(val) else val[0]
                 return val
 
             entities = pick_val(rk_item, "entities")
             answer = pick_val(rk_item, "answer")
             user_prompt_text = pick_val(rk_item, "user_prompt")
 
             # å¦‚æœ reward_kwargs ä¸­æ²¡æœ‰ï¼Œå†ä»é¡¶å±‚ kwargs å…œåº•
             if entities is None:
                 top_entities = kwargs.get("entities")
                 if isinstance(top_entities, list):
                     entities = top_entities[sample_idx] if sample_idx < len(top_entities) else (top_entities[0] if top_entities else None)
                 else:
                     entities = top_entities
             if answer is None:
                 top_answer = kwargs.get("answer")
                 if isinstance(top_answer, list):
                     answer = top_answer[sample_idx] if sample_idx < len(top_answer) else (top_answer[0] if top_answer else None)
                 else:
                     answer = top_answer
             if user_prompt_text is None:
                 top_user_prompt = kwargs.get("user_prompt")
                 if isinstance(top_user_prompt, list):
                     user_prompt_text = top_user_prompt[sample_idx] if sample_idx < len(top_user_prompt) else (top_user_prompt[0] if top_user_prompt else None)
                 else:
                     user_prompt_text = top_user_prompt
 
             if entities is not None:
                 ground_truth = entities
                 data_source = "ner_dapo"
                 gt_source = "reward_kwargs.entities" if rk_item and "entities" in rk_item else "kwargs.entities"
             elif answer is not None:
                 ground_truth = {"answer": answer}
                 data_source = "openai/gsm8k"
                 gt_source = "reward_kwargs.answer" if rk_item and "answer" in rk_item else "kwargs.answer"
             else:
                 ground_truth = {}
                 data_source = "dapo"
                 gt_source = "none"
             
             # è·å–ä¸å½“å‰æ ·æœ¬å¯¹é½çš„ promptï¼ˆé•¿åº¦ç»Ÿè®¡ç”¨ï¼Œä¸å†è®°å½•å†…å®¹ï¼‰
             if isinstance(prompts_list, list):
                 prompt_text = prompts_list[sample_idx] if sample_idx < len(prompts_list) else (prompts_list[0] if prompts_list else None)
             elif isinstance(prompts_list, str):
                 prompt_text = prompts_list
             else:
                 prompt_text = None
 
             # æ„é€ ç²¾ç®€ä¸”å®‰å…¨çš„ extra_info
             extra_info = {
                 "batch_idx": idx,              # å½“å‰completionåœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•
                 "sample_idx": sample_idx,      # å¯¹åº”çš„æ ·æœ¬ç´¢å¼•ï¼ˆå¯¹é½prompts/reward_kwargsï¼‰
                 "completion_length": len(completion),
                 "has_reward_kwargs": isinstance(rk, (list, dict)),
                 "reward_kwargs_type": type(rk).__name__,
                 "reward_kwargs_keys": list(rk_item.keys()) if isinstance(rk_item, dict) else [],
                 "num_prompts": num_prompts,
                 "prompt_length": (len(prompt_text) if isinstance(prompt_text, str) else None),
                 # ä»…è®°å½•NERæŠ½å–ç›®æ ‡æ–‡æœ¬ï¼ˆuserä¾§å†…å®¹ï¼‰ï¼Œä¾¿äºå¯¹é½åˆ†æ
                 "user_prompt_preview": (user_prompt_text[:1000] if isinstance(user_prompt_text, str) else None),
                 "user_prompt_length": (len(user_prompt_text) if isinstance(user_prompt_text, str) else None),
                 "gt_source": gt_source,
             }
             
             # ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°è®¡ç®—åˆ†æ•°
             reward = compute_ner_score_v2(completion, ground_truth, data_source, extra_info)
             
         except Exception as e:
             logging.error(f"âŒ å¥–åŠ±å‡½æ•°è®¡ç®—é”™è¯¯: {e}")
             reward = 0.0
         
         rewards.append(reward)
     
     # åªè®°å½•æ‰¹æ¬¡çº§åˆ«çš„å¥–åŠ±ç»Ÿè®¡ï¼ˆå‡å°‘æ—¥å¿—é¢‘ç‡ï¼‰
     if len(rewards) > 0:
         avg_reward = sum(rewards) / len(rewards)
         min_reward = min(rewards)
         max_reward = max(rewards)
         logging.info(f"ğŸ¯ æ‰¹æ¬¡å¥–åŠ±ç»Ÿè®¡: å¹³å‡={avg_reward:.3f}, æœ€å°={min_reward:.3f}, æœ€å¤§={max_reward:.3f}")
     
     return rewards

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®ï¼ŒåŒ…æ‹¬æ§åˆ¶å°è¾“å‡ºå’Œæ–‡ä»¶è¾“å‡º"""
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    logs_dir = os.path.join(script_dir, "logs")
    
    # æ£€æŸ¥å¹¶åˆ›å»ºlogsç›®å½•
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
        print(f"åˆ›å»ºæ—¥å¿—ç›®å½•: {logs_dir}")
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(logs_dir, f"grpo_log_{timestamp}.log")
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # åˆ›å»ºæ ¼å¼å™¨
    formatter = logging.Formatter(
        "%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S"
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)
    
    print(f"æ—¥å¿—å°†ä¿å­˜åˆ°: {log_file}")
    logging.info(f"å¼€å§‹GRPOè®­ç»ƒ - æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return log_file

def main():
    parser = HfArgumentParser((ModelArguments, DataArguments, GRPOConfig, LoggingArguments))
    model_args, data_args, training_args, logging_args = parser.parse_args_into_dataclasses()

    # è®¾ç½®æ—¥å¿—ï¼ˆåŒ…æ‹¬æ–‡ä»¶è¾“å‡ºï¼‰
    log_file = setup_logging()

    # è®°å½•è®­ç»ƒå‚æ•°
    logging.info("=" * 50)
    logging.info("è®­ç»ƒå‚æ•°é…ç½®:")
    logging.info(f"æ¨¡å‹è·¯å¾„: {model_args.model_name_or_path}")
    logging.info(f"æ•°æ®é›†è·¯å¾„: {data_args.dataset_path}")
    logging.info(f"è¾“å‡ºç›®å½•: {training_args.output_dir}")
    if logging_args.metrics_output_dir:
        logging.info(f"æŒ‡æ ‡JSONç›®å½•: {logging_args.metrics_output_dir}")
    logging.info(f"LoRA r: {model_args.lora_r}")
    logging.info(f"LoRA alpha: {model_args.lora_alpha}")
    logging.info(f"å­¦ä¹ ç‡: {training_args.learning_rate}")
    logging.info(f"æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    logging.info(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {training_args.gradient_accumulation_steps}")
    logging.info(f"æœ€å¤§è®­ç»ƒæ­¥æ•°: {training_args.max_steps}")
    logging.info("=" * 50)

    # åŠ è½½tokenizer
    logging.info("æ­£åœ¨åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        padding_side="left",
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    logging.info("tokenizeråŠ è½½å®Œæˆ")

    # åŠ è½½æ¨¡å‹ï¼ˆå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒç‰ˆæœ¬ï¼‰
    logging.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        dtype=torch.bfloat16,  # ä¿®å¤deprecationè­¦å‘Š
        trust_remote_code=True,
        low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
    )
    logging.info("æ¨¡å‹åŠ è½½å®Œæˆ")

    # åº”ç”¨LoRA
    logging.info("æ­£åœ¨åº”ç”¨LoRAé…ç½®...")
    lora_config = LoraConfig(
        r=model_args.lora_r,
        lora_alpha=model_args.lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_config)
    logging.info("LoRAé…ç½®åº”ç”¨å®Œæˆ")

    # åŠ è½½æ•°æ®
    logging.info("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    raw_data = load_jsonl(data_args.dataset_path)
    dataset = Dataset.from_list(raw_data)
    logging.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} æ¡æ•°æ®")

    # æ•°æ®é¢„å¤„ç†
    logging.info("æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
    def preprocess_function(examples):
        prompts = []
        gt_entities_per_sample = []
        user_prompts_per_sample = []
        for conversation in examples["conversation"]:
            # æ„é€ æç¤ºï¼ˆè·³è¿‡assistantä»¥é¿å…æ³„éœ²ç­”æ¡ˆï¼‰
            prompt_parts = []
            # æ”¶é›†è¯¥å¯¹è¯çš„ground truthï¼ˆä¼˜å…ˆä½¿ç”¨æœ€åä¸€ä¸ªassistantå›ç­”ï¼‰
            last_assistant_entities = []
            last_user_content = ""
            for turn in conversation:
                 role = turn.get("role", "user")
                 content = turn.get("content", "")
                 if role.lower() == "assistant":
                     # è§£æassistantä¸­çš„<ner_result>
                     last_assistant_entities = _extract_entities_from_text(content)
                     continue
                 if role.lower() == "user":
                     # è®°å½•ç”¨äºNERæŠ½å–çš„åŸå§‹ç”¨æˆ·æ–‡æœ¬ï¼ˆå–æœ€åä¸€æ¡userï¼‰
                     last_user_content = content
                 prompt_parts.append(f"<|{role}|>\n{content}\n")
            # åœ¨æœ«å°¾è¿½åŠ assistantèµ·å§‹æ ‡ç­¾ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆ
            prompt = "".join(prompt_parts).strip() + "\n<|assistant|>\n"
            prompts.append(prompt)
            gt_entities_per_sample.append(last_assistant_entities)
            user_prompts_per_sample.append(last_user_content)
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†åˆ«æ„é€  reward_kwargsï¼Œç¡®ä¿ä¸ prompts ä¸€ä¸€å¯¹åº”
        reward_kwargs = []
        n = len(prompts)
        for i in range(n):
            item = {}
            # å°†è§£æå¾—åˆ°çš„ground truthå®ä½“æ”¾å…¥reward_kwargs
            item["entities"] = gt_entities_per_sample[i]
            # åŒæ—¶æºå¸¦userä¾§åŸæ–‡ï¼Œä¾¿äºåœ¨extra_infoä¸­æŒ‰éœ€è®°å½•
            item["user_prompt"] = user_prompts_per_sample[i]
            reward_kwargs.append(item)
        
        return {"prompt": prompts, "reward_kwargs": reward_kwargs}

    processed_dataset = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    logging.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")

    # åˆ›å»ºTrainer
    logging.info("æ­£åœ¨åˆ›å»ºGRPO Trainer...")
    trainer = GRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=processed_dataset,
        reward_funcs=[unified_reward_function],
    )
    logging.info("GRPO Traineråˆ›å»ºå®Œæˆ")

    # ========== å›è°ƒï¼šåœ¨ä¿å­˜æ£€æŸ¥ç‚¹æ—¶å†™å‡ºåŒºé—´å‡å€¼ JSON ==========
    # æŒ‰è¿è¡ŒåŒºåˆ†ï¼šæ”¾åœ¨ metrics_output_dir/<run_id>/ ä¸‹ï¼Œrun_id å– output_dir çš„æœ€åä¸€çº§ç›®å½•å
    base_metrics_dir = logging_args.metrics_output_dir or training_args.output_dir
    run_id = os.path.basename(os.path.normpath(training_args.output_dir)) or "run"
    metrics_dir = os.path.join(base_metrics_dir, run_id)
    logging.info(f"æŒ‡æ ‡JSONæœ€ç»ˆç›®å½•: {metrics_dir}")

    class SaveMetricsCallback(TrainerCallback):
        def __init__(self, metrics_output_dir: str):
            self.reset()
            self.metrics_output_dir = metrics_output_dir

        def reset(self):
            self.loss_sum = 0.0
            self.loss_cnt = 0
            self.reward_sum = 0.0
            self.reward_cnt = 0

        def _maybe_get(self, logs: Dict[str, Any], keys):
            for k in keys:
                v = logs.get(k)
                if isinstance(v, (int, float)):
                    return float(v)
            return None

        def on_log(self, args, state, control, logs=None, **kwargs):
            if not logs:
                return
            # å…¼å®¹ä¸åŒé”®å
            loss_val = self._maybe_get(logs, [
                "loss",
                "train_loss",
                "objective/loss",
            ])
            if loss_val is not None and np.isfinite(loss_val):
                self.loss_sum += float(loss_val)
                self.loss_cnt += 1

            reward_val = self._maybe_get(logs, [
                "rewards/mean",
                "reward/mean",
                "train/reward",
                "avg_reward",
            ])
            if reward_val is not None and np.isfinite(reward_val):
                self.reward_sum += float(reward_val)
                self.reward_cnt += 1

        def _flush_json(self, args, state):
            try:
                os.makedirs(self.metrics_output_dir, exist_ok=True)
                step = int(state.global_step)
                payload = {
                    "global_step": step,
                    "avg_batch_loss": (self.loss_sum / self.loss_cnt) if self.loss_cnt > 0 else None,
                    "avg_batch_reward": (self.reward_sum / self.reward_cnt) if self.reward_cnt > 0 else None,
                }
                fname = os.path.join(self.metrics_output_dir, f"metrics_step_{step}.json")
                with open(fname, "w", encoding="utf-8") as f:
                    json.dump(payload, f, ensure_ascii=False, indent=2)
                logging.info(f"ğŸ“¦ ä¿å­˜åŒºé—´æŒ‡æ ‡: {fname}")
                self.reset()
            except Exception as e:
                logging.warning(f"å†™å‡ºåŒºé—´æŒ‡æ ‡JSONå¤±è´¥: {e}")

        def on_save(self, args, state, control, **kwargs):
            self._flush_json(args, state)

        def on_train_end(self, args, state, control, **kwargs):
            # è®­ç»ƒç»“æŸå…œåº•å†å†™ä¸€æ¬¡ï¼Œé¿å…æœ€åä¸€æ®µæœªå‘½ä¸­ on_save
            if self.loss_cnt > 0 or self.reward_cnt > 0:
                self._flush_json(args, state)

    trainer.add_callback(SaveMetricsCallback(metrics_output_dir=metrics_dir))

    # å¼€å§‹è®­ç»ƒ
    logging.info("å¼€å§‹GRPOè®­ç»ƒ...")
    logging.info(f"è®­ç»ƒå°†è¿›è¡Œ {training_args.max_steps} æ­¥")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ£€æŸ¥ç‚¹æ¢å¤
    resume_from_checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        resume_from_checkpoint = training_args.resume_from_checkpoint
        logging.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from_checkpoint}")
    
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    logging.info("è®­ç»ƒå®Œæˆï¼")

    # ä¿å­˜æ¨¡å‹
    logging.info(f"æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {training_args.output_dir}")
    trainer.save_model(training_args.output_dir)
    tokenizer.save_pretrained(training_args.output_dir)
    logging.info("æ¨¡å‹ä¿å­˜å®Œæˆ")
    logging.info(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

if __name__ == "__main__":
    main()