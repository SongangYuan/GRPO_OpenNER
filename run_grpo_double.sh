#!/usr/bin/env bash
# å•å¡é«˜æ€§èƒ½GRPOè®­ç»ƒï¼ˆä¿®å¤å‚æ•°é”™è¯¯ï¼‰
# 982724
# ====== è·¯å¾„é…ç½® ======
MODEL_NAME_OR_PATH="/private/DAPO/Model/MyModel/LLAMA_7B_SFT_DeepSpeed_Half_Merged"
DATASET_JSONL="/private/DAPO/Data/AnaPileNER/data/Train/RLHF/split_dataset_2.json"
OUTPUT_DIR="/private/DAPO/Model/MyModel/LLAMA_7B_GRPO-double-1-$(date +%Y%m%d_%H%M%S)"
                   
TS=$(date +%Y%m%d_%H%M%S)
TEE_LOG_DIR="/private/DAPO/Data/AnaPileNER/data/Train/RLHF/var_value"
TEE_LOG="$TEE_LOG_DIR/run_grpo_double_${TS}_$$_${RANDOM}.log"
mkdir -p "$TEE_LOG_DIR"

# æ–°å¢žï¼šDeepSpeed é…ç½®ï¼ˆè„šæœ¬åŒç›®å½•ä¸‹çš„ ds_config_zero2.jsonï¼‰
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DS_CONFIG="$SCRIPT_DIR/ds_config_zero2.json"

# ====== é«˜æ€§èƒ½å‚æ•°ï¼ˆé™ä½Žæ˜¾å­˜å ç”¨ï¼‰ ======
PER_DEVICE_BATCH_SIZE=1         # å‡å°æ˜¾å­˜å ç”¨
GRAD_ACC=32                     # ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡ä¸å˜å¯å…ˆä¸æ”¹
MAX_STEP=25000
LORA_R=16                       # æ›´å°LoRAï¼Œè®­ç»ƒæ›´å¿«
LORA_ALPHA=32                   # å¯¹åº”è°ƒæ•´
LR=5e-5                         # æ›´ä¿å®ˆå­¦ä¹ çŽ‡
MAX_PROMPT_LEN=512              # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé™ä½ŽKVç¼“å­˜
MAX_COMPLETION_LEN=256          # é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé™ä½ŽKVç¼“å­˜
NUM_GENERATIONS=2               # æ¯ä¸ªæ ·æœ¬çš„ç”Ÿæˆæ•°ï¼Œè¶Šå¤§å ç”¨è¶Šé«˜

# ====== å…³é”®çŽ¯å¢ƒå˜é‡ ======
export CUDA_VISIBLE_DEVICES=0,1
export TOKENIZERS_PARALLELISM=false
# å‡å°‘ç¢Žç‰‡åŒ–å¯¼è‡´çš„OOMï¼ˆæ¥è‡ªé”™è¯¯æç¤ºå»ºè®®ï¼‰
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

echo "ðŸš€ å¯åŠ¨åŒå¡è®­ç»ƒ (GPU 0,1)..."
echo "LoRA: R=$LORA_R, Alpha=$LORA_ALPHA"
echo "å­¦ä¹ çŽ‡: $LR"
echo "å•å¡æ‰¹æ¬¡: ${PER_DEVICE_BATCH_SIZE} x ${GRAD_ACC} = $((PER_DEVICE_BATCH_SIZE*GRAD_ACC))"
echo "æ€»æ‰¹æ¬¡å¤§å°: $((PER_DEVICE_BATCH_SIZE*GRAD_ACC*2)) (åŒå¡)"
echo "ä½¿ç”¨ DeepSpeed é…ç½®: $DS_CONFIG"
echo "é•¿åº¦è®¾ç½®: prompt=${MAX_PROMPT_LEN}, completion=${MAX_COMPLETION_LEN}, generations=${NUM_GENERATIONS}"

# ====== å¯åŠ¨å‘½ä»¤ ======
torchrun --nproc_per_node=2 /private/DAPO/verl/verl-main/recipe/dapo/main_grpo.py \
--deepspeed "$DS_CONFIG" \
--model_name_or_path "$MODEL_NAME_OR_PATH" \
--dataset_path "$DATASET_JSONL" \
--output_dir "$OUTPUT_DIR" \
--per_device_train_batch_size "$PER_DEVICE_BATCH_SIZE" \
--gradient_accumulation_steps "$GRAD_ACC" \
--max_steps "$MAX_STEP" \
--lora_r "$LORA_R" \
--lora_alpha "$LORA_ALPHA" \
--learning_rate "$LR" \
--bf16 true \
--fp16 false \
--tf32 true \
--optim adamw_torch \
--lr_scheduler_type cosine \
--warmup_ratio 0.05 \
--save_total_limit 5 \
--logging_steps 50 \
--save_steps 2000 \
--dataloader_num_workers 8 \
--gradient_checkpointing true \
--gradient_checkpointing_kwargs '{"use_reentrant": false}' \
--dataloader_pin_memory true \
--max_prompt_length "$MAX_PROMPT_LEN" \
--max_completion_length "$MAX_COMPLETION_LEN" \
--num_generations "$NUM_GENERATIONS" \
--ds3_gather_for_generation false \
--report_to none 2>&1 | tee "$TEE_LOG"
